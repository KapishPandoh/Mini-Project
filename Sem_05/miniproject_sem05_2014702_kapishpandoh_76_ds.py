# -*- coding: utf-8 -*-
"""MiniProject_Sem05_2014702_KapishPandoh_76_DS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14QZzYy6vTWfaB3ANyCPq3D9vUavr6icw

<center><h1> Classifying Ships in Satellite Imagery using CNN </h1></center>

<img src='shipimg.jpg' width='800' height='700' />

* **Dataset** : [Classifying Ships in Satellite Imagery](https://www.kaggle.com/rhammell/ships-in-satellite-imagery?select=shipsnet)

**Content** :

- The dataset consists of image chips extracted from Planet satellite imagery collected over the San Francisco Bay and San Pedro Bay areas of California. 

- It includes `4000 80x80` RGB images labeled with either a "ship" or "no-ship" classification.

**Data** :

- `label`: Valued 1 or 0, representing the "ship" class and "no-ship" class, respectively.


- `scene id` : The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene.


- `data` : The pixel values are stored in a column in the data frame titled “data.”

`The dataset is also distributed as a JSON formatted text file shipsnet.json. The loaded object contains data, label, scene_ids, and location lists.`

* The pixel value data for each `80x80 RGB image` is stored as a list of `19200 integers` within the data list. 

* The first 6400 entries contain the red channel values, the next 6400 the green, and the final 6400 the blue.

## Import Libraries
"""

import pandas as pd
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Conv2D

# used for reduce data dimensionality
from tensorflow.keras.layers import MaxPooling2D

from tensorflow.keras.layers import Dropout

# number of units proportional to input
from tensorflow.keras.layers import Flatten

#regular dense connected layer
from tensorflow.keras.layers import Dense

from tensorflow.keras.layers import Activation

from tensorflow.keras.optimizers import Adam

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_auc_score, roc_curve

from tensorflow.keras.models import load_model

import warnings
warnings.filterwarnings("ignore")

"""## Load Dataset"""

dataset = pd.read_json("shipsnet.json")
dataset

dataset['data'].shape

"""## Transform Data

**As is, these pixel values aren’t ready to be processed by a CNN. Instead, the new data is converted to a NumPy array and divided by 255 to normalize the values.**

**All 19,200 values should now be some value between 0 and 1.**

**Next the data is reshaped to 80 x 80 x 3 matrix so that it’s formatted as a picture**
"""

dataset['normalized_data'] = dataset['data'].apply(lambda x : (np.array(x)/255).reshape(80,80,3))

dataset['normalized_data'].shape

dataset.head()

"""## Split Data"""

X = dataset["normalized_data"]
y = dataset["labels"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

X_train.shape

X_train

"""**Unfortunately, Pandas Series aren’t accepted in TensorFlow, so the training and testing data are converted into arrays**"""

X_train = np.array([i  for i in X_train])
X_test = np.array([i  for i in X_test])

y_train = np.array([i  for i in y_train])
y_test = np.array([i  for i in y_test])

X_train.shape

X_train.shape[1:]

"""## Train CNN model

**The CNN is initialized as a sequential model, which ensures each layer receives one input and one output.**
"""

model = Sequential()

"""**Conv2D: convulutional layer. In the first layer the input is shaped (80, 80, 3): a 3D-object of 80x80x3, or a matrix 80x80 for each channel of information (R, G, B)**

The first layer is a convolution layer which uses a “ReLu” activation function and expects an input tensor of 80 x 80 x 3, the exact dimensions of the training images. The 32 represents the dimensionality of the output of the layer and the (3, 3) represents the size of the convolution window, 3px x 3px in this case.

The next layer added is for max pooling, which takes a pool size of 2 x 2.
"""

# 1st convulsion layer
model.add(
           Conv2D(
                   filters     = 32                ,
                   kernel_size = (3,3)             ,
                   input_shape = X_train.shape[1:] ,
                   activation  = 'relu'            ,
                  )
           )


model.add(
           MaxPooling2D(pool_size=(2,2))
         )

# hidden layer 1

model.add(
           Conv2D(
                   filters     = 64                ,
                   kernel_size = (3,3)             ,
                   activation  = 'relu'            ,
                  )
           )


model.add(
           MaxPooling2D(pool_size=(2,2))
         )

model.add(
           Dropout(0.25)
         )

# hidden layer 2

model.add(
           Conv2D(
                   filters     = 64                ,
                   kernel_size = (3,3)             ,
                   activation  = 'relu'            ,
                  )
           )

# hidden layer 3

model.add(
           Conv2D(
                   filters     = 64                ,
                   kernel_size = (3,3)             ,
                   activation  = 'relu'            ,
                  )
           )

model.add(
           MaxPooling2D(pool_size=(2,2))
         )

model.add(
           Dropout(0.25)
         )

# Flattens the input into a 1D tensor
# this line simply flattens the tensor into a 1 dimension, which will make processing easier.

model.add(
           Flatten()
         )

# Makes the input more readable for classification

model.add(
            Dense(
                   64, 
                   activation='relu'
                 )
         )

# again called dropout for some extra regularization

model.add(
           Dropout(0.5)
         )

# add on that final dense layer 
# so that our output is equal to the number of classes 

model.add(
           Dense(1)
         )

# Final activation function
# Finally an activation layer is added which tells the whether or not to fire the neuron.

model.add(Activation('sigmoid'))

model.summary()

# Compile the model

'''
Use binary_crossentropy because there are only 2 classes present
'''

model.compile(
               loss      = 'binary_crossentropy' , 
               optimizer = 'rmsprop'             , 
               metrics   = ['accuracy']
             )

"""The epochs argument essentially tells the model how many iterations to go through.

There’s a diminishing margin of return for setting epochs. A higher number will generally return a better accuracy, but each additional gain in accuracy will decrease until it approaches the maximum amount of accuracy the dataset can produce. 

Additionally, more epochs will take longer to run.
"""

model_history = model.fit(
                           X_train                           , 
                           y_train                           ,
                           #batch_size      = 32              ,
                           epochs          = 10              ,
                           validation_data = (X_test, y_test),
                           #shuffle         = True
                         )

"""After returning the number of samples and giving the time to train the epoch, the line will also return the loss and accuracy of the model on its own training set of images as well as its loss and accuracy for the validation set.

In this case, on the 10th epoch, the model achieved `98.40% accuracy` on its own **training images** and `98.00% accuracy` on **validation images** images the model had never seen before.
"""

val_loss ,val_acc = model.evaluate(X_test ,y_test)
print("Validation loss     : {:.2f}".format(val_loss) )
print("Validation accuracy : {:.2f} %".format(val_acc*100))

train_acc = model_history.history['accuracy']
val_acc   = model_history.history['val_accuracy']

plt.plot(train_acc ,label='Train Accuracy')
plt.plot(val_acc   ,label='Test  Accuracy')

plt.xlabel("Epochs")
plt.ylabel("Accuracy")

plt.legend()

plt.title("Model Accuracy")

train_loss = model_history.history['loss']
val_loss   = model_history.history['val_loss']

plt.plot(train_loss ,color='red'  ,marker='.' ,label='Train Loss')
plt.plot(val_loss   ,color='blue' ,marker='.' ,label='Test  Loss')

plt.xlabel("Epochs")
plt.ylabel("Loss")

plt.legend()

plt.title("Model Loss")

"""## Evaluation"""

y_pred = model.predict(X_test)
y_pred = (y_pred>0.5)

accuracy_score(y_test,y_pred)

confusion_matrix(y_test,y_pred)

print(classification_report(y_test, y_pred))

y_prob = model.predict(X_test)

auc = roc_auc_score(y_test,y_prob)
auc

def plot_roc(y_test ,y_prob ,model):
    
    fpr, tpr, thr = roc_curve(y_test, y_prob)
    
    plt.plot(fpr ,tpr ,'k-')
    plt.plot([0, 1], [0, 1], 'k--', linewidth=0.5)
    
    plt.grid(True)
    
    plt.title("ROC Curve : " + model)

plot_roc(y_test, y_prob, 'Convolutional Neural Network')

"""## Saving Model

The save method simply takes the name of the path to save it as a H5 file.
"""

model.save("MiniProject__Sem05.h5")

"""# Load a model"""

new_model = load_model("MiniProject__Sem05.h5")
